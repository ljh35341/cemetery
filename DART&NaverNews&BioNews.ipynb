{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 안 쓰는 이유\n",
    "- notepad에 저장하는 함수로 수정해서 사용하게 됐다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\n",
    "    '셀트리온':'00413046',\n",
    "    '에이치엘비':'00199252'\n",
    "}\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "for key in dict:\n",
    "    \n",
    "    url = 'http://dart.fss.or.kr/api/companyRSS.xml?crpCd='+dict[key]\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    items = soup.find_all(\"item\")\n",
    "    for item in items:\n",
    "        print(item.title.text)\n",
    "        print(item.guid.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaverNews\n",
    "- url에서 \"&sort=date\"이므로 최신순 정렬이다.\n",
    "- url에서 news를 blog로 수정하면 blog 자료가 나온다.\n",
    "- CLIENT_ID와 CLIENT_SECRE은 가렸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = <클라이언트 아이디>\n",
    "CLIENT_SECRET = <시크릿 아이디>\n",
    "SEARCH_WORD = \"에이치엘비\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "client_id = CLIENT_ID\n",
    "client_secret = CLIENT_SECRET\n",
    "encText = urllib.parse.quote(SEARCH_WORD)\n",
    "url = \"https://openapi.naver.com/v1/search/news?query=\" + encText + \"&sort=date\"\n",
    "\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode==200):\n",
    "    response_body = response.read()\n",
    "    res = response_body.decode('utf-8')\n",
    "    json_res = json.loads(res)\n",
    "    for item in json_res[\"items\"]:\n",
    "        print(item[\"title\"])\n",
    "        print(item[\"link\"])\n",
    "else:\n",
    "    print(\"Error Code:\" + rescode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = {\n",
    "    \"메디파나\" : \"http://www.medipana.com/news/news_list_new.asp?Page=1&sWord=&sDate=&MainKind=C&NewsKind=67&vCount=20&vKind=1\",\n",
    "    \"약업닷컴\" : \"https://www.yakup.com/news/index.html?cat=12\",\n",
    "    \"바이오스펙테이터\" : \"http://www.biospectator.com/section/section_list.php?MID=11100\"\n",
    "}\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "for key in url.keys():\n",
    "    res = requests.get(url[key])\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "\n",
    "    if key == \"메디파나\":\n",
    "        titles = soup.find_all(\"span\", class_=[\"tit\", \"infor\"]) #다중 클래스 선택\n",
    "        i = 0\n",
    "        print(\"메디파나\")\n",
    "        for title in titles:\n",
    "            print(title.text)\n",
    "            if i % 2 == 1:\n",
    "                print(\"http://www.medipana.com\"+title.parent[\"href\"][2:])\n",
    "            i += 1\n",
    "        print(\"\")\n",
    "\n",
    "    elif key == \"약업닷컴\":\n",
    "        titles = soup.find_all(\"h6\")\n",
    "        print(\"약업닷컴\")\n",
    "        for title in titles:\n",
    "            print(title.text)\n",
    "            print(title.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling.text)\n",
    "            print(\"https://www.yakup.com/\"+title.a[\"href\"])\n",
    "        print(\"\")\n",
    "\n",
    "    elif key == \"바이오스펙테이터\":\n",
    "        titles = soup.find_all(\"strong\", class_=\"article_tit\")\n",
    "        print(\"바이오스펙테이터\")\n",
    "        for title in titles:\n",
    "            print(title.text.strip())\n",
    "            print(title.next_sibling.next_sibling.next_sibling.next_sibling.find(\"span\", class_=\"date\").text)\n",
    "            print(\"http://www.biospectator.com/\"+title.a[\"href\"])\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
